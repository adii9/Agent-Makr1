{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927cf25b",
   "metadata": {},
   "source": [
    "# GitHub MCP Agent with LangGraph & LangChain\n",
    "\n",
    "This notebook demonstrates how to build an autonomous agent using LangGraph and LangChain that can:\n",
    "- Use configurable LLM connections (Ollama and Gemini)\n",
    "- Perform basic operations (starting with addition)\n",
    "- Integrate with GitHub using Model Context Protocol (MCP)\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup Project Environment with UV\n",
    "2. Install Required Dependencies  \n",
    "3. Configure LLM Connections (Ollama and Gemini)\n",
    "4. Import Required Libraries\n",
    "5. Create Basic Addition Tool\n",
    "6. Build LangGraph Agent Architecture\n",
    "7. Define Agent State and Nodes\n",
    "8. Create Agent Workflow\n",
    "9. Test Basic Addition Agent\n",
    "10. GitHub MCP Integration Setup\n",
    "11. Extend Agent with GitHub Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8841512",
   "metadata": {},
   "source": [
    "## 1. Setup Project Environment with UV\n",
    "\n",
    "We're using UV for fast Python package management. The project structure has been initialized with the following components:\n",
    "\n",
    "- `pyproject.toml` - Project configuration and dependencies\n",
    "- `.env.example` - Template for environment variables\n",
    "- `.env` - Local environment configuration\n",
    "- `agent_notebook.ipynb` - This notebook\n",
    "\n",
    "Key benefits of using UV:\n",
    "- Faster dependency resolution and installation\n",
    "- Better dependency management\n",
    "- Virtual environment management\n",
    "- Compatible with pip and PyPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291f10d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Verify UV installation and show project structure\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check UV version\n",
    "try:\n",
    "    result = subprocess.run(['uv', '--version'], capture_output=True, text=True)\n",
    "    print(f\"UV Version: {result.stdout.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"UV is not installed. Please install UV first.\")\n",
    "\n",
    "# Show current project structure\n",
    "project_root = Path.cwd()\n",
    "print(f\"\\nProject Root: {project_root}\")\n",
    "print(\"\\nProject Structure:\")\n",
    "for item in sorted(project_root.iterdir()):\n",
    "    if item.name.startswith('.') and item.name not in ['.env', '.env.example']:\n",
    "        continue\n",
    "    print(f\"  {item.name}\")\n",
    "\n",
    "# Show pyproject.toml content\n",
    "pyproject_path = project_root / \"pyproject.toml\"\n",
    "if pyproject_path.exists():\n",
    "    print(f\"\\nContents of pyproject.toml:\")\n",
    "    with open(pyproject_path, 'r') as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e784bd0",
   "metadata": {},
   "source": [
    "## 2. Install Required Dependencies\n",
    "\n",
    "Our project includes the following key dependencies:\n",
    "\n",
    "- **LangChain & LangGraph**: Core framework for building the agent\n",
    "- **Ollama**: Local LLM integration\n",
    "- **Google Generative AI**: Gemini API integration  \n",
    "- **MCP**: Model Context Protocol for GitHub integration\n",
    "- **Jupyter**: For interactive development\n",
    "- **Python-dotenv**: Environment variable management\n",
    "\n",
    "Dependencies have been pre-installed via `uv sync`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e4d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify key packages are installed\n",
    "import pkg_resources\n",
    "\n",
    "key_packages = [\n",
    "    'langchain',\n",
    "    'langchain-core',\n",
    "    'langgraph',\n",
    "    'langchain-ollama',\n",
    "    'langchain-google-genai',\n",
    "    'jupyter',\n",
    "    'python-dotenv',\n",
    "    'pydantic',\n",
    "    'httpx',\n",
    "    'mcp'\n",
    "]\n",
    "\n",
    "print(\"üì¶ Installed Key Packages:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for package in key_packages:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(package).version\n",
    "        print(f\"‚úÖ {package:<25} v{version}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"‚ùå {package:<25} NOT INSTALLED\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"‚ú® Package verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865671e",
   "metadata": {},
   "source": [
    "## 3. Configure LLM Connections (Ollama and Gemini)\n",
    "\n",
    "We'll create a configurable system that allows switching between different LLM providers:\n",
    "\n",
    "- **Ollama**: For local LLM inference (privacy-focused, offline capable)\n",
    "- **Gemini**: For Google's powerful cloud-based models\n",
    "\n",
    "The configuration will be managed through environment variables, making it easy to switch providers without code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b12e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal, Optional\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class LLMConfig(BaseModel):\n",
    "    \"\"\"Configuration for LLM connections\"\"\"\n",
    "    provider: Literal[\"ollama\", \"gemini\"] = \"ollama\"\n",
    "\n",
    "    # Ollama configuration\n",
    "    ollama_base_url: str = \"http://localhost:11434\"\n",
    "    ollama_model: str = \"llama2\"\n",
    "\n",
    "    # Gemini configuration\n",
    "    google_api_key: Optional[str] = None\n",
    "    gemini_model: str = \"gemini-pro\"\n",
    "\n",
    "def load_llm_config() -> LLMConfig:\n",
    "    \"\"\"Load LLM configuration from environment variables\"\"\"\n",
    "    return LLMConfig(\n",
    "        provider=os.getenv(\"DEFAULT_LLM_PROVIDER\", \"ollama\"),\n",
    "        ollama_base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),\n",
    "        ollama_model=os.getenv(\"OLLAMA_MODEL\", \"llama2\"),\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        gemini_model=os.getenv(\"GEMINI_MODEL\", \"gemini-pro\")\n",
    "    )\n",
    "\n",
    "def create_llm(config: LLMConfig):\n",
    "    \"\"\"Create LLM instance based on configuration\"\"\"\n",
    "    if config.provider == \"ollama\":\n",
    "        print(f\"ü¶ô Initializing Ollama with model: {config.ollama_model}\")\n",
    "        return ChatOllama(\n",
    "            base_url=config.ollama_base_url,\n",
    "            model=config.ollama_model,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    elif config.provider == \"gemini\":\n",
    "        if not config.google_api_key:\n",
    "            raise ValueError(\"Google API key is required for Gemini provider\")\n",
    "        print(f\"ü§ñ Initializing Gemini with model: {config.gemini_model}\")\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model=config.gemini_model,\n",
    "            google_api_key=config.google_api_key,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported LLM provider: {config.provider}\")\n",
    "\n",
    "# Initialize configuration\n",
    "config = load_llm_config()\n",
    "print(f\"üìã LLM Configuration:\")\n",
    "print(f\"  Provider: {config.provider}\")\n",
    "print(f\"  Ollama URL: {config.ollama_base_url}\")\n",
    "print(f\"  Ollama Model: {config.ollama_model}\")\n",
    "print(f\"  Gemini Model: {config.gemini_model}\")\n",
    "print(f\"  Google API Key: {'‚úÖ Set' if config.google_api_key else '‚ùå Not set'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfdcd7",
   "metadata": {},
   "source": [
    "## 4. Import Required Libraries\n",
    "\n",
    "Now let's import all the necessary libraries for building our LangGraph agent with LangChain integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core LangChain and LangGraph imports\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseRetriever\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "from typing import Annotated, Dict, Any, List, Sequence, TypedDict\n",
    "import operator\n",
    "\n",
    "# Additional utilities\n",
    "from pydantic import BaseModel, Field\n",
    "import asyncio\n",
    "\n",
    "print(\"üìö All required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf4641",
   "metadata": {},
   "source": [
    "## 5. Create Basic Addition Tool\n",
    "\n",
    "Let's start with a simple tool that can add two numbers. This will serve as our foundation before extending to more complex GitHub operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers together.\n",
    "\n",
    "    Args:\n",
    "        a: First number to add\n",
    "        b: Second number to add\n",
    "\n",
    "    Returns:\n",
    "        The sum of a and b\n",
    "    \"\"\"\n",
    "    result = a + b\n",
    "    print(f\"üî¢ Adding {a} + {b} = {result}\")\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(x: int, y: int) -> int:\n",
    "    \"\"\"Multiply two numbers together.\n",
    "\n",
    "    Args:\n",
    "        x: First number to multiply\n",
    "        y: Second number to multiply\n",
    "\n",
    "    Returns:\n",
    "        The product of x and y\n",
    "    \"\"\"\n",
    "    result = x * y\n",
    "    print(f\"‚úñÔ∏è Multiplying {x} √ó {y} = {result}\")\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def get_calculation_info() -> str:\n",
    "    \"\"\"Get information about available mathematical operations.\n",
    "\n",
    "    Returns:\n",
    "        Information about supported operations\n",
    "    \"\"\"\n",
    "    info = \"\"\"\n",
    "    üìä Available Mathematical Operations:\n",
    "    - Addition: Use add_numbers(a, b) to add two integers\n",
    "    - Multiplication: Use multiply_numbers(x, y) to multiply two integers\n",
    "\n",
    "    Example usage:\n",
    "    - \"Add 5 and 3\" ‚Üí add_numbers(5, 3) ‚Üí 8\n",
    "    - \"Multiply 4 by 6\" ‚Üí multiply_numbers(4, 6) ‚Üí 24\n",
    "    \"\"\"\n",
    "    return info\n",
    "\n",
    "# Create tools list\n",
    "basic_tools = [add_numbers, multiply_numbers, get_calculation_info]\n",
    "\n",
    "print(\"üîß Basic mathematical tools created:\")\n",
    "for tool in basic_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")\n",
    "print(f\"\\n‚úÖ Total tools available: {len(basic_tools)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9bfc1",
   "metadata": {},
   "source": [
    "## 6. Build LangGraph Agent Architecture\n",
    "\n",
    "Now we'll create the core agent architecture using LangGraph. This includes:\n",
    "\n",
    "- **State Management**: Defining how the agent tracks conversation and tool usage\n",
    "- **LLM Integration**: Connecting our configurable LLM to the agent\n",
    "- **Tool Binding**: Making our tools available to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01347713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM with our configuration\n",
    "try:\n",
    "    llm = create_llm(config)\n",
    "    print(f\"‚úÖ LLM initialized successfully: {config.provider}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize LLM: {e}\")\n",
    "    print(\"üîÑ Falling back to a mock LLM for demonstration\")\n",
    "    # Create a simple mock for demo purposes\n",
    "    class MockLLM:\n",
    "        def invoke(self, messages):\n",
    "            return AIMessage(content=\"I would process this with a real LLM\")\n",
    "        def bind_tools(self, tools):\n",
    "            return self\n",
    "    llm = MockLLM()\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools(basic_tools)\n",
    "\n",
    "print(f\"üîß LLM bound with {len(basic_tools)} tools\")\n",
    "print(\"üèóÔ∏è Agent architecture foundation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a4218",
   "metadata": {},
   "source": [
    "## 7. Define Agent State and Nodes\n",
    "\n",
    "The agent state will track the conversation messages and any additional context needed for operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79628049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent state\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "class BaseMessage:\n",
    "    \"\"\"Base message class for compatibility\"\"\"\n",
    "    def __init__(self, content: str, type: str = \"human\"):\n",
    "        self.content = content\n",
    "        self.type = type\n",
    "\n",
    "# Define the assistant node (LLM call)\n",
    "def call_model(state: AgentState):\n",
    "    \"\"\"Call the LLM with the current conversation state\"\"\"\n",
    "    print(\"ü§ñ Assistant is thinking...\")\n",
    "\n",
    "    # Get the messages from state\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    try:\n",
    "        # Call the LLM\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        print(f\"üí≠ Assistant response: {response.content}\")\n",
    "\n",
    "        # Return the response to add to the state\n",
    "        return {\"messages\": [response]}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error calling model: {e}\")\n",
    "        # Return an error message\n",
    "        error_response = AIMessage(content=f\"I encountered an error: {str(e)}\")\n",
    "        return {\"messages\": [error_response]}\n",
    "\n",
    "print(\"üèóÔ∏è Agent state and nodes defined:\")\n",
    "print(\"  - AgentState: Tracks conversation messages\")\n",
    "print(\"  - call_model: Handles LLM interactions\")\n",
    "print(\"‚úÖ Ready to build the workflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838b779",
   "metadata": {},
   "source": [
    "## 8. Create Agent Workflow\n",
    "\n",
    "Now we'll create the complete LangGraph workflow that orchestrates the conversation flow between the LLM and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d910ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Create tool node\n",
    "tool_node = ToolNode(basic_tools)\n",
    "\n",
    "# Add nodes to the workflow\n",
    "workflow.add_node(\"assistant\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"assistant\")\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,  # This decides whether to call tools or end\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"__end__\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add edge from tools back to assistant\n",
    "workflow.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# Add memory for conversation persistence\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the workflow\n",
    "try:\n",
    "    agent = workflow.compile(checkpointer=memory)\n",
    "    print(\"‚úÖ Agent workflow compiled successfully!\")\n",
    "    print(\"üîÑ Workflow structure:\")\n",
    "    print(\"  1. User input ‚Üí Assistant (LLM)\")\n",
    "    print(\"  2. Assistant decides: Use tools or respond\")\n",
    "    print(\"  3. If tools needed: Execute ‚Üí Return to Assistant\")\n",
    "    print(\"  4. Final response to user\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error compiling workflow: {e}\")\n",
    "    agent = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e6795",
   "metadata": {},
   "source": [
    "## 9. Test Basic Addition Agent\n",
    "\n",
    "Let's test our agent with some basic mathematical operations to ensure everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(query: str, thread_id: str = \"test-thread\"):\n",
    "    \"\"\"Test the agent with a query\"\"\"\n",
    "    print(f\"üß™ Testing: '{query}'\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if agent is None:\n",
    "        print(\"‚ùå Agent not available - workflow compilation failed\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Create the input message\n",
    "        input_message = HumanMessage(content=query)\n",
    "\n",
    "        # Run the agent\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "        # Stream the agent's response\n",
    "        for chunk in agent.stream(\n",
    "            {\"messages\": [input_message]},\n",
    "            config=config\n",
    "        ):\n",
    "            print(f\"üì§ Chunk: {chunk}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during agent execution: {e}\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "\n",
    "# Test cases\n",
    "test_queries = [\n",
    "    \"Add 15 and 27\",\n",
    "    \"What's 8 multiplied by 9?\",\n",
    "    \"Can you tell me what mathematical operations you can perform?\",\n",
    "    \"Calculate 100 plus 200, then multiply the result by 3\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Starting Agent Tests...\")\n",
    "print()\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"Test {i}:\")\n",
    "    test_agent(query, f\"test-thread-{i}\")\n",
    "\n",
    "print(\"‚úÖ All tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f641f0",
   "metadata": {},
   "source": [
    "## 10. GitHub MCP Integration Setup\n",
    "\n",
    "Now let's set up the GitHub Model Context Protocol (MCP) integration. This will allow our agent to interact with GitHub repositories, issues, pull requests, and more.\n",
    "\n",
    "**Note**: This section requires:\n",
    "1. A GitHub personal access token\n",
    "2. Repository permissions appropriate for your use case\n",
    "3. The MCP GitHub server setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ebabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub MCP Configuration\n",
    "import httpx\n",
    "from typing import Optional\n",
    "\n",
    "class GitHubMCPConfig(BaseModel):\n",
    "    \"\"\"Configuration for GitHub MCP integration\"\"\"\n",
    "    github_token: Optional[str] = None\n",
    "    github_repo: Optional[str] = None\n",
    "    base_url: str = \"https://api.github.com\"\n",
    "\n",
    "def load_github_config() -> GitHubMCPConfig:\n",
    "    \"\"\"Load GitHub configuration from environment\"\"\"\n",
    "    return GitHubMCPConfig(\n",
    "        github_token=os.getenv(\"GITHUB_TOKEN\"),\n",
    "        github_repo=os.getenv(\"GITHUB_REPO\"),\n",
    "        base_url=os.getenv(\"GITHUB_BASE_URL\", \"https://api.github.com\")\n",
    "    )\n",
    "\n",
    "# GitHub Tools for the agent\n",
    "@tool\n",
    "def get_repo_info(repo_name: str) -> str:\n",
    "    \"\"\"Get basic information about a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        repo_name: Repository name in format 'owner/repo'\n",
    "\n",
    "    Returns:\n",
    "        Repository information as a formatted string\n",
    "    \"\"\"\n",
    "    github_config = load_github_config()\n",
    "\n",
    "    if not github_config.github_token:\n",
    "        return \"‚ùå GitHub token not configured. Please set GITHUB_TOKEN environment variable.\"\n",
    "\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"token {github_config.github_token}\",\n",
    "            \"Accept\": \"application/vnd.github.v3+json\"\n",
    "        }\n",
    "\n",
    "        url = f\"{github_config.base_url}/repos/{repo_name}\"\n",
    "\n",
    "        with httpx.Client() as client:\n",
    "            response = client.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            repo_data = response.json()\n",
    "            info = f\"\"\"\n",
    "            üìÅ Repository: {repo_data['full_name']}\n",
    "            üìù Description: {repo_data.get('description', 'No description')}\n",
    "            ‚≠ê Stars: {repo_data['stargazers_count']}\n",
    "            üç¥ Forks: {repo_data['forks_count']}\n",
    "            üìä Language: {repo_data.get('language', 'Not specified')}\n",
    "            üîó URL: {repo_data['html_url']}\n",
    "            \"\"\"\n",
    "            return info\n",
    "        else:\n",
    "            return f\"‚ùå Failed to fetch repository info: {response.status_code}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error fetching repository info: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def list_repo_issues(repo_name: str, limit: int = 5) -> str:\n",
    "    \"\"\"List recent issues from a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        repo_name: Repository name in format 'owner/repo'\n",
    "        limit: Maximum number of issues to return (default: 5)\n",
    "\n",
    "    Returns:\n",
    "        List of recent issues as a formatted string\n",
    "    \"\"\"\n",
    "    github_config = load_github_config()\n",
    "\n",
    "    if not github_config.github_token:\n",
    "        return \"‚ùå GitHub token not configured.\"\n",
    "\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"token {github_config.github_token}\",\n",
    "            \"Accept\": \"application/vnd.github.v3+json\"\n",
    "        }\n",
    "\n",
    "        url = f\"{github_config.base_url}/repos/{repo_name}/issues\"\n",
    "        params = {\"state\": \"open\", \"per_page\": limit}\n",
    "\n",
    "        with httpx.Client() as client:\n",
    "            response = client.get(url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            issues = response.json()\n",
    "\n",
    "            if not issues:\n",
    "                return f\"üì≠ No open issues found in {repo_name}\"\n",
    "\n",
    "            result = f\"üìã Recent Issues in {repo_name}:\\\\n\\\\n\"\n",
    "            for issue in issues:\n",
    "                result += f\"#{issue['number']}: {issue['title']}\\\\n\"\n",
    "                result += f\"   üë§ Created by: {issue['user']['login']}\\\\n\"\n",
    "                result += f\"   üìÖ Created: {issue['created_at'][:10]}\\\\n\"\n",
    "                result += f\"   üè∑Ô∏è Labels: {', '.join([label['name'] for label in issue['labels']])}\\\\n\\\\n\"\n",
    "\n",
    "            return result\n",
    "        else:\n",
    "            return f\"‚ùå Failed to fetch issues: {response.status_code}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error fetching issues: {str(e)}\"\n",
    "\n",
    "# Create GitHub tools list\n",
    "github_tools = [get_repo_info, list_repo_issues]\n",
    "\n",
    "# Load GitHub configuration\n",
    "github_config = load_github_config()\n",
    "print(\"üêô GitHub MCP Configuration:\")\n",
    "print(f\"  Token: {'‚úÖ Set' if github_config.github_token else '‚ùå Not set'}\")\n",
    "print(f\"  Default Repo: {github_config.github_repo or 'Not set'}\")\n",
    "print(f\"  Base URL: {github_config.base_url}\")\n",
    "print(f\"\\\\nüîß GitHub tools available: {len(github_tools)}\")\n",
    "for tool in github_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146ce88",
   "metadata": {},
   "source": [
    "## 11. Extend Agent with GitHub Capabilities\n",
    "\n",
    "Now let's create an enhanced version of our agent that includes both mathematical operations and GitHub repository management capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4dbcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all tools for the enhanced agent\n",
    "all_tools = basic_tools + github_tools\n",
    "\n",
    "print(f\"üîß Enhanced Agent Tools ({len(all_tools)} total):\")\n",
    "print(\"\\\\nMathematical Operations:\")\n",
    "for tool in basic_tools:\n",
    "    print(f\"  - {tool.name}\")\n",
    "\n",
    "print(\"\\\\nGitHub Operations:\")\n",
    "for tool in github_tools:\n",
    "    print(f\"  - {tool.name}\")\n",
    "\n",
    "# Create enhanced agent\n",
    "try:\n",
    "    # Bind all tools to LLM\n",
    "    enhanced_llm = llm.bind_tools(all_tools)\n",
    "\n",
    "    # Create new workflow for enhanced agent\n",
    "    enhanced_workflow = StateGraph(AgentState)\n",
    "\n",
    "    # Enhanced assistant node\n",
    "    def enhanced_call_model(state: AgentState):\n",
    "        \"\"\"Enhanced assistant with GitHub capabilities\"\"\"\n",
    "        print(\"ü§ñ Enhanced Assistant is processing...\")\n",
    "        messages = state[\"messages\"]\n",
    "\n",
    "        try:\n",
    "            response = enhanced_llm.invoke(messages)\n",
    "            print(f\"üí≠ Enhanced response generated\")\n",
    "            return {\"messages\": [response]}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in enhanced model: {e}\")\n",
    "            error_response = AIMessage(\n",
    "                content=f\"I encountered an error: {str(e)}. I can help with math operations and GitHub repository queries.\"\n",
    "            )\n",
    "            return {\"messages\": [error_response]}\n",
    "\n",
    "    # Create enhanced tool node\n",
    "    enhanced_tool_node = ToolNode(all_tools)\n",
    "\n",
    "    # Build enhanced workflow\n",
    "    enhanced_workflow.add_node(\"assistant\", enhanced_call_model)\n",
    "    enhanced_workflow.add_node(\"tools\", enhanced_tool_node)\n",
    "    enhanced_workflow.set_entry_point(\"assistant\")\n",
    "\n",
    "    enhanced_workflow.add_conditional_edges(\n",
    "        \"assistant\",\n",
    "        tools_condition,\n",
    "        {\"tools\": \"tools\", \"__end__\": END}\n",
    "    )\n",
    "    enhanced_workflow.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "    # Compile enhanced agent\n",
    "    enhanced_agent = enhanced_workflow.compile(checkpointer=MemorySaver())\n",
    "    print(\"‚úÖ Enhanced agent created successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating enhanced agent: {e}\")\n",
    "    enhanced_agent = None\n",
    "\n",
    "# Test the enhanced agent\n",
    "def test_enhanced_agent(query: str, thread_id: str = \"enhanced-test\"):\n",
    "    \"\"\"Test the enhanced agent with GitHub and math capabilities\"\"\"\n",
    "    print(f\"üöÄ Enhanced Test: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if enhanced_agent is None:\n",
    "        print(\"‚ùå Enhanced agent not available\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        input_message = HumanMessage(content=query)\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "        for chunk in enhanced_agent.stream({\"messages\": [input_message]}, config=config):\n",
    "            print(f\"üì§ Enhanced Chunk: {chunk}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in enhanced agent test: {e}\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "# Enhanced test cases\n",
    "enhanced_test_queries = [\n",
    "    \"Add 25 and 35\",\n",
    "    \"Get information about the repository 'microsoft/vscode'\",\n",
    "    \"List the recent issues from 'facebook/react'\",\n",
    "    \"Calculate 12 multiplied by 8, then tell me about the 'python/cpython' repository\"\n",
    "]\n",
    "\n",
    "print(\"\\\\nüéØ Testing Enhanced Agent with GitHub + Math capabilities...\")\n",
    "print()\n",
    "\n",
    "for i, query in enumerate(enhanced_test_queries, 1):\n",
    "    print(f\"Enhanced Test {i}:\")\n",
    "    test_enhanced_agent(query, f\"enhanced-test-{i}\")\n",
    "\n",
    "print(\"üéâ Enhanced agent testing completed!\")\n",
    "print(\"\\\\nüèÜ Agent Capabilities Summary:\")\n",
    "print(\"  ‚úÖ Mathematical operations (add, multiply)\")\n",
    "print(\"  ‚úÖ GitHub repository information\")\n",
    "print(\"  ‚úÖ GitHub issues listing\")\n",
    "print(\"  ‚úÖ Configurable LLM backend (Ollama/Gemini)\")\n",
    "print(\"  ‚úÖ Conversation memory and state management\")\n",
    "print(\"  ‚úÖ Extensible tool architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bbb115",
   "metadata": {},
   "source": [
    "## üéØ Next Steps and Usage Instructions\n",
    "\n",
    "### Configuration\n",
    "\n",
    "1. **For Ollama (Local LLM)**:\n",
    "   ```bash\n",
    "   # Install Ollama: https://ollama.ai\n",
    "   ollama pull llama2  # or your preferred model\n",
    "   ```\n",
    "\n",
    "2. **For Gemini (Cloud LLM)**:\n",
    "   ```bash\n",
    "   # Set your Google API key in .env\n",
    "   GOOGLE_API_KEY=your_api_key_here\n",
    "   DEFAULT_LLM_PROVIDER=gemini\n",
    "   ```\n",
    "\n",
    "3. **For GitHub Integration**:\n",
    "   ```bash\n",
    "   # Set your GitHub token in .env\n",
    "   GITHUB_TOKEN=your_github_token_here\n",
    "   GITHUB_REPO=your_username/your_repo\n",
    "   ```\n",
    "\n",
    "### Extending the Agent\n",
    "\n",
    "You can easily add more tools:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def your_custom_tool(param: str) -> str:\n",
    "    \\\"\\\"\\\"Description of your tool\\\"\\\"\\\"\n",
    "    # Your tool logic here\n",
    "    return result\n",
    "\n",
    "# Add to tools list and rebuild agent\n",
    "```\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "- Consider using a more robust state store (Redis, PostgreSQL)\n",
    "- Add error handling and retry logic\n",
    "- Implement rate limiting for API calls\n",
    "- Add logging and monitoring\n",
    "- Use environment-specific configurations\n",
    "\n",
    "### Architecture Benefits\n",
    "\n",
    "- **Modular**: Easy to add/remove tools\n",
    "- **Configurable**: Switch LLM providers easily  \n",
    "- **Scalable**: LangGraph handles complex workflows\n",
    "- **Maintainable**: Clear separation of concerns\n",
    "- **Extensible**: Built for future enhancements\n",
    "\n",
    "**üöÄ Your autonomous GitHub MCP agent is ready to use!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
